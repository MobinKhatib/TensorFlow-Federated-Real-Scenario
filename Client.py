# -*- coding: utf-8 -*-
"""federated_learning_for_DRED_yooho_(1).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1LoBhx17HswWU4jUYJ9iGiYeSoMzfCiI5

# Federated Learning for DRED
"""

#!pip install tensorflow_federated==0.75.0

#import os
#os.kill(os.getpid(), 9) #For reconnecting runtime

#@test {"skip": true}

#!pip install --quiet --upgrade tensorflow-federated

# Commented out IPython magic to ensure Python compatibility.
# %load_ext tensorboard

import collections

import numpy as np
import tensorflow as tf
import tensorflow_federated as tff
import pandas as pd
import pandas as pd
from sklearn.model_selection import train_test_split
import tensorflow as tf
import tensorflow_federated as tff
import greet_pb2_grpc
import greet_pb2
import time
import grpc
import os

np.random.seed(0)

tff.federated_computation(lambda: 'Hello, World!')()

"""### Preparing the input data

Federated learning requires a federated data set,
i.e., a collection of data from multiple users. Federated data is typically
non-[i.i.d.]
which poses a unique set of challenges.



Here's how we can load it:
"""


# Load the dataset while specifying data types for each column
file_path = '/home/ubuntumobin/MyCodes/new_dataset.csv'
dtypes = {'Client': int, 'Feature1': float, 'Feature2': float, 'Feature3': float, 'Feature4': float, 'Feature5': float, 'Label': int}
num_rows = 166353
nrows = int(num_rows/2)
df = pd.read_csv(file_path, header=0, dtype=dtypes, nrows=nrows)
# This make all clients equal to 0
#df['Client'] = 0
# Print the first few rows of the DataFrame
print(df.head())

#column 2 for our purpose chosen
client_id_colname = 'Client' # the column that represents client ID
# split client id into train and test clients
df[client_id_colname] = df[client_id_colname].astype(int) # astype make values of the column to int wheter if they are or not.
client_ids = df[client_id_colname].unique() # If we have multiple similar values,unique make them unique! And turn it back

#train_client_ids = client_ids.sample(frac=0.5).tolist()

# Convert client_ids to a Pandas Series
client_ids_series = pd.Series(client_ids)
# Sample 70% of the client IDs for training
train_client_ids = client_ids_series.sample(frac=1).tolist()
#train_client_ids = client_ids_series.sample(frac=0.7).tolist()
#test_client_ids = [x for x in client_ids if x not in train_client_ids]

#print(df.columns)

"""Print client_ids"""

#client_ids = df[client_id_colname].unique()
#print(client_ids)
#df[client_id_colname] = df[client_id_colname].astype(int)
#print(df[client_id_colname])

SHUFFLE_BUFFER = 1000
#NUM_EPOCHS = 5
NUM_EPOCHS = 15

def serializable_dataset_fn(client_id):
  # a function which takes a client_id and returns a tf.data.Dataset for that client
  client_data = df[df[client_id_colname] == client_id] # Make each client together each of them separately as a dataset
  # The below line: This is just for feature 1 : {'client1': {'features': <tf.Tensor: shape=(), dtype=int32, numpy=1>, 'labels': <tf.Tensor: shape=(), dtype=int32, numpy=0>}, 'client2': {'features': <tf.Tensor: shape=(), dtype=int32, numpy=5>, 'labels': <tf.Tensor: shape=(), dtype=int32, numpy=1>}}
  #It make them like this
  dataset = tf.data.Dataset.from_tensor_slices(client_data.to_dict('list'))
  dataset = dataset.shuffle(SHUFFLE_BUFFER).batch(1).repeat(NUM_EPOCHS) # Here is batch 1: Each step of optimization operates on a single datapoint at a time. Batching helps in efficient processing
  # repeat(NUM_EPOCHS) : How many times it operates on our dataset from start to end
  # shuffle(SHUFFLE_BUFFER) : It chose datapoints in the number of SHUFFLE_BUFFER from different places in dataset(after this we have batch operation)
  return dataset

# from_clients_and_fn  to  from_clients_and_tf_fn
# Substitute create_tf_dataset_for_client_fn  with  serializable_dataset_fn parameter.(Because it is in tensorflow and it need to divide to train and test)
train_data = tff.simulation.datasets.TestClientData.from_clients_and_tf_fn(

        client_ids=train_client_ids,
        serializable_dataset_fn=serializable_dataset_fn
    )
#test_data = tff.simulation.datasets.TestClientData.from_clients_and_tf_fn(
#        client_ids=test_client_ids,
#        serializable_dataset_fn=serializable_dataset_fn
#    )

"""We are creating a dataset for each client dynamically based on their client ID, the size of the data produced by serializable_dataset_fn may vary depending on the client's data. Therefore, TensorFlow sets the shape of the tensors to None along those dimensions to accommodate this variability. This allows TensorFlow to handle tensors of different sizes without needing to specify a fixed shape in advance."""

#train_data.element_type_structure

example_dataset = train_data.serializable_dataset_fn(
        train_data.client_ids[0]
    )
#print(type(example_dataset))
example_element = iter(example_dataset).__next__()
#print(example_element)

example_dataset = train_data.create_tf_dataset_for_client(
    train_data.client_ids[0])

example_element = next(iter(example_dataset))
#example_element['Feature2'].numpy()[0]

#example_element['Label'].numpy()[0] #because different shuffling,we have different values

"""Federated data is typically non-i.i.d., users typically have different distributions of data depending on usage patterns. Some clients may have fewer training examples on device, suffering from data paucity locally, while some clients will have more than enough training examples.

It's important to note that this deep analysis of a client's data is only available to us because this is a simulation environment where all the data is available to us locally. In a real production federated environment we would not be able to inspect a single client's data (For this part we should change our code it's simple we just initiallize the weights and biases in level 1 and for the next levels we give this properties which are aggregated and sent by global server for inputs of level 2,3,etc.)

### Preprocessing the input data
"""

NUM_CLIENTS = 1 # If we have one client, We'll see it is still ok!
# We can change it. It is not matter now.
NUM_EPOCHS = 15
BATCH_SIZE = 20
SHUFFLE_BUFFER = 100
PREFETCH_BUFFER = 10 # Prefetching allows TensorFlow to overlap the time spent waiting for data with the time spent training the model,
#potentially reducing the overall training time.

def preprocess(dataset):

    def batch_format_fn(element):
        """Flatten a batch `pixels` and return the features as an `OrderedDict`."""
        # Concatenate and reshape all features into a 2D array ([-1,1] makes all columns like features and label horizontal)
        x = tf.concat([
            tf.reshape(element['Feature1'], [-1, 1]),
            tf.reshape(element['Feature2'], [-1, 1]),
            tf.reshape(element['Feature3'], [-1, 1]),
            tf.reshape(element['Feature4'], [-1, 1]),
            tf.reshape(element['Feature5'], [-1, 1])
        ], axis=1)

        return collections.OrderedDict(
            x=x,
            y=tf.reshape(element['Label'], [-1, 1])
        )

    return dataset.repeat(NUM_EPOCHS).shuffle(SHUFFLE_BUFFER, seed=1).batch(
        BATCH_SIZE).map(batch_format_fn).prefetch(PREFETCH_BUFFER) # map: The output of the map function is a new dataset where each element has been transformed according to the specified function after its reshape

"""Verify this work:"""

preprocessed_example_dataset = preprocess(example_dataset)

sample_batch = tf.nest.map_structure(lambda x: x.numpy(),
                                     next(iter(preprocessed_example_dataset)))

#sample_batch

"""We have almost all the building blocks in place to construct federated data
sets.

One of the ways to feed federated data to TFF in a simulation is simply as a
Python list, with each element of the list holding the data of an individual
user, whether as a list or as a `tf.data.Dataset`.

Here's a simple helper function that will construct a list of datasets from the
given set of users as an input to a round of training or evaluation.
"""

def make_federated_data(client_data, client_ids):
  return [
      preprocess(client_data.create_tf_dataset_for_client(x))
      for x in client_ids
  ]

"""Now, how do we choose clients?

In a typical federated training scenario, we are dealing with potentially a very
large population of user devices, only a fraction of which may be available for
training at a given point in time. This is the case,when the
client devices are ESP32's or RasPi(doeesn't matter which) that participate in training only when plugged
into a power source, off a metered network, and otherwise idle.

Of course, we are in a simulation environment, and all the data is locally
available. Typically then, when running simulations, we would simply sample a
random subset of the clients to be involved in each round of training, generally
different in each round.


subsets of clients in each round can take a while, and it would be impractical
to have to run hundreds of rounds in this interactive tutorial.

What we'll do instead is sample the set of clients once, and
reuse the same set across rounds to speed up convergence (intentionally
over-fitting to these few user's data).
"""

sample_clients = train_data.client_ids[0:NUM_CLIENTS]

federated_train_data = make_federated_data(train_data, sample_clients)

#print(f'Number of client datasets: {len(federated_train_data)}')
#print(f'First dataset: {federated_train_data[0]}')

"""## Creating a model with Keras

An example of a simple model that will suffice for our needs.
"""

input_shape = (5,)
def create_keras_model():
    return tf.keras.models.Sequential([
        tf.keras.layers.InputLayer(input_shape=input_shape),
        # Dense Layer = 5 : Because our Labels have 5 values[0,1,2,3,4]
        tf.keras.layers.Dense(5, kernel_initializer='zeros'),# We just add one dense layer here,dense layer is 5 here decrease or increase may become underfit or overfit
        tf.keras.layers.Softmax() # Output Layer
    ])

# Create a new model that takes the input and outputs the output of the dense layer
# This way we don't have a separate output(which we make it with softmax before)
#This make the similation easier
dense_layer_model = tf.keras.models.Sequential([
    tf.keras.layers.InputLayer(input_shape=input_shape),
    create_keras_model().layers[1]  # Extract the dense layer from the original model
])

"""We have seen below the main idea of Federated Averaging Algorithm from:

Communication efficient,Communication-Efficient Learning of Deep Networks from Decentralized Data,H. Brendan McMahan and Eider Moore and Daniel Ramage and Seth Hampson and Blaise Agüera y Arcas,2023,1602.05629,arXiv,cs.LG

(which we are gonna use this method for our purpose)

**Note:** we do not compile the model yet. The loss, metrics, and optimizers are introduced later.

In order to use any model with TFF, it needs to be wrapped in an instance of the
`tff.learning.models.VariableModel` interface, which exposes methods to stamp the model's forward pass, metadata properties, etc., similarly to Keras, but also introduces additional elements, such as ways to control the process of computing federated metrics.
Let's not worry about this for now; with a Keras model like the
one we've just defined above, we can have TFF wrap it for you by invoking
`tff.learning.models.from_keras_model`, passing the model and **a sample data batch as arguments:**
"""

def model_fn():
  # We _must_ create a new model here, and _not_ capture it from an external
  # scope. TFF will call this within different graph contexts.
  keras_model = create_keras_model()
  return tff.learning.models.from_keras_model(
      keras_model,
      input_spec=preprocessed_example_dataset.element_spec,
      loss=tf.keras.losses.SparseCategoricalCrossentropy(),
      metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])

"""## Training the model on federated data

Now that we have a model wrapped as `tff.learning.models.VariableModel` for use with TFF, we can let TFF construct a Federated Averaging algorithm by invoking the helper
function `tff.learning.algorithms.build_weighted_fed_avg`, as follows.

Argument needs to be a constructor (such as `model_fn`
above), not an already-constructed instance, so that the construction of our
model can happen in a context controlled by TFF.
One critical note on the Federated Averaging algorithm below, there are **2**
optimizers: a **_client_optimizer_ and a _server_optimizer_.** The
_client_optimizer_ is only used to compute local model updates on each client.
The _server_optimizer_ applies the averaged update to the global model at the
server. In particular, this means that the choice of optimizer and learning rate
used may need to be different than the ones we have used to train the model on
a standard i.i.d. dataset. We starting with regular SGD, possibly with
a smaller learning rate than usual.

TFF has constructed a pair of *federated computations* and
packaged them into a `tff.templates.IterativeProcess` in which these computations
are available as a pair of properties `initialize` and `next`.

In a nutshell, *federated computations* are programs in TFF's internal language
that can express various federated algorithms. In this
case, the two computations generated and packed into `iterative_process`
implement

It is a goal of TFF to define computations in a way that they could be executed
in real federated learning settings, but currently only local execution
simulation runtime is implemented.

 `initialize` computation:(We initialize here all properties zero for just first tunning.)
"""

training_process = tff.learning.algorithms.build_weighted_fed_avg(
    model_fn,
    client_optimizer_fn=lambda: tf.keras.optimizers.SGD(learning_rate=0.01),
    server_optimizer_fn=lambda: tf.keras.optimizers.SGD(learning_rate=1.0))

#print(training_process.initialize.type_signature.formatted_representation())

# We just need send weights, Biases are not important
# Manually initialize the finalizer part
# We just replace the trainable model weights in finalizer and initialize it in global_model_weights

file_path = 'client_weight.txt'  # Path to weights file

# Ensure that the file exists by opening it in write mode
# If the file already exists, it will be truncated (its contents deleted)
with open(file_path, 'w') as file:
    pass  # Do nothing, just ensure the file is created

def read_weights_from_file(file_path):
    with open(file_path, 'r') as file:
        for line in file:
            weights_array = np.fromstring(line.strip(), dtype=float, sep=',')
    return weights_array.reshape(5, 5)

try:
    if os.path.getsize(file_path) != 0:
        finalizer = [
            0,  # Integer
            read_weights_from_file(file_path),  # Array of float32 with shape (5, 5)
            np.zeros(5, dtype=np.float32)  # Array of float32 with shape (5,)
        ]
    else:
        finalizer = [
            0,  # Integer
            np.zeros((5, 5), dtype=np.float32),  # Array of float32 with shape (5, 5)
            np.zeros(5, dtype=np.float32)  # Array of float32 with shape (5,)
        ]
except FileNotFoundError:
    print(f"File '{file_path}' not found.")
tff.learning.models.ModelWeights
# Initialize eval_state with the manually initialized finalizer
eval_state = tff.learning.templates.LearningAlgorithmState(
    global_model_weights=tff.learning.models.ModelWeights(
        trainable=[np.zeros((5,5), dtype=np.float32), np.zeros(5, dtype=np.float32)],
        non_trainable=[]
    ),
    distributor=(),
    client_work=(),
    aggregator=tff.structure.collections.OrderedDict([('value_sum_process', ()), ('weight_sum_process', ())]),
    finalizer=finalizer
)

#print(training_process.get_model_weights.type_signature.parameter)

#print(eval_state)

"""We can
recognize that the server state consists of a `global_model_weights` (the initial model parameters for DRED that will be distributed to all devices), some empty parameters (like `distributor`, which governs the server-to-client communication) and a `finalizer` component. This last one governs the logic that the server uses to update its model at the end of a round, and contains an integer representing how many rounds of FedAvg have occurred.

Let's invoke the `initialize` computation to construct the server state.

The second of the pair of federated computations, `next`, represents a single
round of Federated Averaging, which consists of pushing the server state
(including the model parameters) to the clients, on-device training on their
local data, collecting and averaging model updates, and producing a new updated
model at the server.

Conceptually, you can think of `next` as having a functional type signature that
looks as follows.

```
SERVER_STATE, FEDERATED_DATA -> SERVER_STATE, TRAINING_METRICS
```

In particular, one should think about `next()` not as being a function that runs on a server, 
but rather being a declarative functional representation of the entire decentralized computation - 
some of the inputs are provided by the server (`SERVER_STATE`), 
but each participating device contributes its own local dataset.

We can use the federated data we've already generated above for a sample of users.
"""

"""We run a few more rounds. As noted earlier(new randomly selected sample of
users for each round in order to simulate a realistic deployment in which users
continuously come and go, but in this interactive notebook, for the sake of
demonstration we'll just reuse the same users, so that the system converges
quickly.)
"""

NUM_ROUNDS = 25
train_state = training_process.initialize()
for round_num in range(1, NUM_ROUNDS + 1):
  result = training_process.next(eval_state, federated_train_data)
  train_state = result.state
  train_metrics = result.metrics
  print('round {:2d}, metrics={}'.format(round_num, train_metrics))

print(train_state.global_model_weights.trainable[0])
updated_global_weights = np.ndarray.flatten(train_state.global_model_weights.trainable[0])
print(updated_global_weights)

def get_client_requests():
        name = input("Please enter a name (or nothing to stop chatting): ")
        hello_request = greet_pb2.HelloRequest(greeting = "Hello", name = name)
        hello_request.weights.extend(updated_global_weights)
        
        return hello_request

def get_client_stream_requests():
    while True:
        name = input("Please enter a name (or nothing to stop chatting): ")

        if name == "":
            break
        hello_request = greet_pb2.HelloRequest(greeting = "Hello", name = name)
        hello_request.weights.extend(updated_global_weights)
        
        yield hello_request
        time.sleep(1)

def run():
    with grpc.insecure_channel('localhost:50051') as channel: # The port we want to connect it to(server)
        stub = greet_pb2_grpc.GreeterStub(channel) # call grpc and connect to channel
        print("1. SayHello - Client and Server One Time Send ")
        print("2. InteractingHello - Both Streaming")
        rpc_call = input("Which rpc would you like to make: ")
        if rpc_call == "1":
            response = stub.SayHello(get_client_requests())
            print("InteractingHello Response Received: ")
            print(response.weights)
            # Convert repeated field to a comma-separated string
            weights_str = ','.join(map(str, response.weights))
            # Create and write to a file
            with open("client_weight.txt", "w") as file:
                file.write(weights_str + '\n')
            print(response.message)
            print("Weights:")
            print(response.weights)
        elif rpc_call == "2":
            responses = stub.InteractingHello(get_client_stream_requests())
            for response in responses:
                print("InteractingHello Response Received: ")
                print(response.weights)
                # Convert repeated field to a comma-separated string
                weights_str = ','.join(map(str, response.weights))
                # Create and write to a file
                with open("client_weight.txt", "w") as file:
                    file.write(weights_str + '\n')
                print(response.message)
                print("Weights:")
                print(response.weights)
                
if __name__ == "__main__":
    run()